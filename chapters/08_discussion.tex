\chapter{Discussion}
\label{chap:discussion}
\section{Restatement of the Research Problem}
The research problem addressed in this thesis revolved around the challenges of semantic segmentation in \ac{CV}, particularly in the context of highly unbalanced datasets of varying difficulty. The primary issue lies in optimizing loss functions, which is crucial in training effective segmentation models. Traditional approaches often rely on a single loss function, which might be optimal for all types of data and tasks and can lead to suboptimal performance and limit the potential of semantic segmentation models.

The research addressed the limitations of using a single loss function by developing and implementing a methodology that merges multiple loss functions, thereby enhancing segmentation performance. This process entailed several stages. First, a segmentation architecture based on the U-Net model was developed. Following this, baseline models were trained using a variety of distinct loss functions. An in-depth analysis of these loss functions was conducted to identify combinations that yielded promising results, leading to a series of merge strategies that were subsequently integrated into a fully automatic loss merging experimentation framework as a key output of this research.

Furthermore, the research extended to conducting extensive tests to gain insights into suitable loss combinations and merging strategies for optimal performance. 

\section{Key Findings and Interpretation}
\chapref{chap:results} presented a comprehensive range of results derived from three distinct datasets, each characterized by varying class counts and degrees of complexity. The \acf{MFD} may be considered the simplest, followed by the \acf{SLD}, and ultimately, the \acf{IDRID}, which posed the most significant challenge. Several factors can influence the difficulty level of predictions for specific datasets, such as dataset size, class imbalance, or characteristics like the location, shape, and similarity of regions of interest. Related to these characteristics, predictions become more challenging as the variation for these properties increases. Among the datasets evaluated in this study, the \ac{IDRID} shows significant variation in these attributes, in addition to its small size and high-class imbalance. The detailed properties of each dataset were introduced in the \nameref{sec:datasets} section.

The presented findings resulted by following a model production cycle defined in \secref{subsec:segmentation_framework} and further described in \chapref{chap:experimental_setup}, which includes dataset properties, loss combination, and merge strategy-specific configurations, preprocessing, training, validation, and testing setups, monitoring functionalities and an ablation setup. Upon this setup described, the results were generated and analyzed.

\input{tables/quantitative_results_summaryV2}

Table \ref{tab:quantitative_results_summaryV2} provides a summary of the quantitative results obtained for the three datasets that were evaluated. The column labeled \squote{Difficulty} reflects the complexity, in ascending order, involved in training the dataset. The \squote{Baseline-Exceeding Count} column indicates the number of times a model, using a specific combination of loss functions and merging techniques, surpasses the highest average baseline result in terms of the \ac{IoU} score. The \squote{Top Average (Specific)} column showcases the aggregate average of some promising loss-merge combinations and compares them with the top baseline results. The reasoning for these trainings was to reaffirm the validity by providing an equal iteration count as the baseline, which was not feasible for the entire range of combinations due to a lack of computational resources. These combinations are presented in the appendix in \secref{subsec:specific_combination_medaka}, \secref{subsec:specific_combination_melanoma} and \secref{subsec:specific_combination_idrid} and correspond to the fifth configurations for each dataset as defined in table \ref{tab:final_configuration_list}. Lastly, the \squote{Top Model} column highlights the highest individual scores achieved using merging techniques and baseline trainings.

\subsection{Quantitative findings}
The \nameref{sec:quantitative_results} section confirmed the correlation of dataset difficulty with the measured performance quantitatively. The \ac{MFD} achieved the best overall results, followed by the \ac{SLD}, with the poorest results attributed to the \ac{IDRID}.

\subsubsection*{Loss combinations}
With the primary goal to demonstrate whether combining losses improves performance over baseline models trained with a single loss, the analysis revealed that this was possible for a range of combinations, as presented in the corresponding \squote{Overall Performance} subsection of each dataset section. The evidence suggests that double and triple loss combinations can generate top-performing results unattainable by the single loss baseline models.

\textbf{\emph{The \acf{MFD}}} displayed the highest degree of performance improvement. The proposed framework introduced 23 distinct double and 18 triple combinations, each averaging higher performance than the top two baseline models in table \ref{tab:baseline_medaka_short}. Table \ref{tab:quantitative_results_summaryV2} lists this number in the \squote{Baseline-Exceeding Count} section. Additionally, we can see a superior performance of the \squote{Top Average (Specific)} column and \squote{Top Model} column over the top baseline models displayed.

\textbf{\emph{The \acf{SLD}}} performance was significantly lower in numbers of models that outperformed the baseline. Both double and triple combinations saw eight models on average showing superior performance than the top-performing baseline model no. 2, trained with the \ac{FL} and displayed in \ref{tab:baseline_skin_lesion_short}. The \squote{Top Average (Specific)} displays a pretty substantial increase using the combination of \ac{CE} and \ac{DL} along the arithmetic averaging strategy. On the other hand, the \squote{Top Model} columns example presents a result which increased just lightly compared to the baseline.

\textbf{\emph{The \acf{IDRID}}} performance enhancement over the top-performing baseline model on the \ac{IDRID} was achieved for nine models using a double loss and just three models using a triple loss combination even if individual models as illustrated in \squote{Top Average (Specific)} and \squote{Top Model} from the table \ref{tab:quantitative_results_summaryV2} displayed quite superior performance.

These findings coincide with the dataset difficulty, implying that the more accessible a dataset is in size, distribution, shape, and location, the higher the number of loss combinations that can surpass the top-performing baseline models. It was observed further that double loss combinations outperformed triple loss combinations across all datasets, with combinations of \ac{CE}, \ac{DL}, and \ac{CE}, \ac{TL} showing the highest frequency of models which were capable of surpassing baseline models. For triple losses, the combination of \ac{FL}, \ac{DL}, \ac{HL} exhibited the highest number of outperforming baseline models on average, followed by the combination of \ac{FL},\ac{TL}, \ac{HL}, and \ac{CE},\ac{DL}, \ac{HL}.

\subsubsection*{Merge strategies}
Regarding merge strategies, the Normalized Weighted Sum (NWS) yielded the best results in outperforming top baseline models across all datasets, followed by arithmetic averaging (AVG) and the regular weighted Sum (WS). In addition to the discrete merging strategies, a continuous merging strategy was proposed, which included the search through a continuous hyperparameter space. While this strategy, on average, performed relatively poorly as low-performing models were included in the results, the hyperparameter analysis evaluated a clear tendency for higher values of the variable \texttt{pbm\_I} to produce much better results. Looking at \figref{annealing}, we can see that the descending curve would reflect these findings, which is displayed in the image (a) and (b) where \texttt{pbm\_I} is close or equals one. These findings are dataset specific but also specific to in which order different loss combinations are used. Nevertheless, they provide a good indicator for the user to narrow down the search space fast after some initial training on subsets of the data. For the parameter \texttt{pbm\_alpha}, some stable results could be observed around values between two and six. A clear drop in the \ac{IoU} metric is apparent for larger values. Looking at \figref{annealing} again, we can see that if \texttt{pbm\_alpha} is chosen too high, the weight contribution for each loss combination is getting very low, which seems to produce a much lower performance.

\subsection{Qualitative findings}
The \nameref{sec:qualitative_results} section visually showcased some top-performing models. Visual results generally can facilitate debugging but also provide an intuitive understanding of how metrics such as \ac{PPV}, \ac{TPR}, or others impact the performance. The qualitative section highlighted differences in these metrics and additionally referred to the subsection \squote{\ac{PPV} vs. \ac{TPR}} in the appendix, summarizing the prevalence of \ac{PPV} versus \ac{TPR} quantitatively. The reference indicated which metrics achieved better results for each loss combination. While averaging strategies like harmonic averaging (HARMONIC) and arithmetic averaging (AVG) typically produced results with higher precision, extreme point strategies such as the min strategy (MIN) and the max strategy (MAX) resulted in models with higher recall.

Even if these results seem to be pretty straightforward, it is crucial to acknowledge that the MIN and MAX strategies performed relatively poorly in terms of \ac{IoU} which encourages us to question and further investigate the results of figures \ref{ppv_vs_tpr_medaka}, \ref{ppv_vs_tpr_melanoma} and \ref{ppv_vs_tpr_idrid} which reaffirms the necessity of highlighting different aspects of a model's performance to avoid producing misleading results. 

The qualitative analysis also demonstrated the performance increase of underrepresented classes present in the \ac{IDRID}. It was shown that some classes increased their performance by more than 30\% over the top-performing baseline model.

\subsection{Computation time evaluation}
The \nameref{sec:computation_time} section presented the average training time per model, signifying a high computational effort required to train the proposed framework. It was observed that the effort to train a single model at least doubled when using a loss combination. This finding highlights the importance of techniques that can reduce computation time, leading to the introduction of the ablation study, where the potential of a dataset subset to yield similar results was analyzed.

\subsection{Ablation study implications}
The \nameref{subsec:ablation_setup} section introduced a method designed to analyze the similarity in performance of a set of results from different data subsets. Intuitively, this method compares the overall results of each loss and merge combination across different data subsets. It measures the average and standard deviation of ratios from all loss and merge combinations, where the average ratio denotes whether the performance has increased or decreased when using fewer data. Meanwhile, the standard deviation of these ratios signifies how similar the results are across all observed combinations. Heatmaps, which visually present the calculated results indicating further a high similarity across the subsets, have been created for three data subsets and are included in the appendix. The ablation study was applied exclusively to the Medaka Fish and \acf{SLD} and revealed in \secref{sec:ablation_results} a high similarity across models trained on subsets of data specifically for the \ac{SLD}.